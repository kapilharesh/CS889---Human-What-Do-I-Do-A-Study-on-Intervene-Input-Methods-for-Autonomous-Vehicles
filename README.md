## CS889 - Human What Do I Do-A Study on Intervene Input Methods for Autonomous Vehicles
CS889 Course Paper - Winter 2018

As autonomous vehicles get more advanced, we need to ensure that users are able to be confident in these vehicles and trust them, in order for them to be a success. This is especially hard in some autonomy levels such as the Level 3 Conditional Automation, where participants don't have to pay attention all the time but need to be available for the car if a request to intervene is invoked by the car. One way we see autonomous vehicles being able to build trust with users is to allow users to know that given a situation where there is a request to intervene, the user can input their decision quickly - especially in the case of cooperative driving environments.

Inspired by research done by Walch et. al \cite{Walch2016}, we present a study that looks into the use of touch screens, voice commands and gesture controls in the input of responses to intervene requests in autonomous vehicles. We ran a user study with 5 participants from the class, where each participant undergoes a 4 condition x 3 input method within-subject experiment. At the end of the study that allows us to see input speed, perceived and actual effectiveness as well as ease of use, we see that while voice commands are typically the fastest, touch screens are typically more accurate and effective (both perceived and based on quantitative data), a well as the easiest in most cases. Voice commands however were easier and more effective than gesture control, which was the worst in all measurable way and based on participant feedback as well. We hope to see this study repeated in the future with a larger, more diverse population of participants to get an even better idea on how these input methods perform when used by people who are older and less technologically savvy.
